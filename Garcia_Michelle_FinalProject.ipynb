{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a54ed679",
   "metadata": {},
   "source": [
    "# Final Project \n",
    "## Music Genre Classification with KNN, CNN, & CNN-RNN\n",
    "### Michelle Garcia\n",
    "### November 19, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17fff29",
   "metadata": {},
   "source": [
    "### Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc07d5",
   "metadata": {},
   "source": [
    "Assume we find someone's relic of an iPod and we want to know what kind of music they like without listening to all the audio tracks. We will test a variety of neural networks -  KNN, CNN, CNN-RNN in tandem, and parallel CNN-RNN -  to categorize each audio file in a certain music genre such as Disco, hip-hop, etc that are among the ten genres in the dataset we are using. \n",
    "\n",
    "Music Genre Recognition is an important field of research in Music Information Retrieval (MIR). A music genre is a conventional category that identifies some pieces of music as belonging to a shared tradition or set of conventions, i.e. it depicts the style of music. Music Genre Recognition involves making use of features such as spectrograms, MFCCâ€™s for predicting the genre of music.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1605126a",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80015e",
   "metadata": {},
   "source": [
    "I used the GITZAN dataset. The dataset consists of 1000 audio tracks, each 30 seconds long. It contains ten genres, or classes: blues, classical, country, disco, hip-hop, jazz, reggae, rock, metal, and pop. Each genre consists of 100 sound clips. Each audio file is in .wav format (extension).\n",
    "\n",
    "I downloaded the dataset from Kaggle because the version from GitHub was giving me issues. I am using a Linux virtual environment in Ubuntu to deploy my jupyter notebook. I uploaded the zip file to my file directory. I wrote a small script to unzip the files and store them into a directory. \n",
    "\n",
    "    import zipfile as zf\n",
    "    files = zf.ZipFile(\"archive (7).zip\", 'r')\n",
    "    files.extractall('DirectoryToExtract')\n",
    "    files.close()\n",
    "\n",
    "There is file error among the jazz clips so I deleted jazz000054.wav from the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033c4a2",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bcf194",
   "metadata": {},
   "source": [
    "Before we move to load dataset and model building we will install certain libraries. Among the standard Python libraries, we will use the python speech feature library to extract features as well as to load the dataset in the WAV format. We will use librosa to extract features for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4455e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.io.wavfile as wav\n",
    "from python_speech_features import mfcc\n",
    "from tempfile import TemporaryFile\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import operator\n",
    "from IPython.display import SVG\n",
    "from os.path import isfile\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, LSTM, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Conv2D, BatchNormalization, Lambda\n",
    "from keras.layers import ELU\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import regularizers\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from xgboost import plot_tree, plot_importance\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f6bdb",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d4751",
   "metadata": {},
   "source": [
    "KNN works by calculating distance and finding the K number of neighbors. We will implement a function that will accept training data, current instances, and the required number of neighbors. Our KNN algorithm will find the distance of each point with every other point in the training data before finding all the nearest K neighbors and returning all neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd977809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to get distance between feature vectors and find neighbors\n",
    "def getNeighbors(trainingset, instance, k):\n",
    "    distances = []\n",
    "    for x in range(len(trainingset)):\n",
    "        dist = distance(trainingset[x], instance, k) + distance(instance,trainingset[x],k)\n",
    "        distances.append((trainingset[x][2], dist))\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][0])\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a8ae51",
   "metadata": {},
   "source": [
    "Once list of neighbors is established we need to find the class that has the maximum neighbors. Initiate a dictionary to store the class and its respective count of neighbors before creating the frequency map, sorting the map in descending order based on neighbors count, and returning the first class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802490b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to identify the nearest neighbors\n",
    "def nearestclass(neighbors):\n",
    "    classVote = {}\n",
    "    \n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x]\n",
    "        if response in classVote:\n",
    "            classVote[response] += 1\n",
    "        else:\n",
    "            classVote[response] = 1\n",
    "            \n",
    "    sorter = sorted(classVote.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorter[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1315989",
   "metadata": {},
   "source": [
    "We need a function for a simple accuracy calculator to evaluate the model to check the accuracy and performance of the algorithm we build. The calculator takes the total number of correct predictions divided by a total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(testSet, prediction):\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x][-1] == prediction[x]:\n",
    "            correct += 1\n",
    "    return 1.0 * correct / len(testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9a377",
   "metadata": {},
   "source": [
    "### Load Data and Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e7bd4",
   "metadata": {},
   "source": [
    "Our next phase of the project implementation will requre we will load the data from all the 10 folders of respective categories and extract features from each audio file and save the extracted features in binary form in DAT extension format.\n",
    "\n",
    "Audio features are classified into 3 categories high-level, mid-level, and low-level audio features.\n",
    "\n",
    "    High-level features include music lyrics like chords, rhythm, melody, etc.\n",
    "    Mid-level features include beat level attributes, pitch-like fluctuation patterns, and MFCCs.\n",
    "    Low-level features include energy, a zero-crossing rate which are statistical measures that get extracted from audio during feature extraction.\n",
    "    \n",
    "We will use Mel Frequency Cepstral Coefficients (MFCC) to generate these features. To use MFCCs to extract mid-level and low-level audio features we must progress through the following steps:\n",
    "    1. Divide the audio file into small-small frames which are near about 20 to 40 ms long.\n",
    "    2. Identify and extract different frequencies from each frame. \n",
    "    3. To discard any type of noise, take discrete cosine transform (DCT) of the frequencies. \n",
    "\n",
    "MFCC brings all these for us which we have already imported from the python speech feature library. Later we will use librosa to visualize our MFCC data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ba6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each category folder, read the audio file, extract the MFCC feature, \n",
    "# and dump it in a binary file using the pickle module\n",
    "directory = \"./Data/genres_original\"\n",
    "f = open(\"mydataset.dat\", \"wb\")\n",
    "i = 0\n",
    "for folder in os.listdir(directory):\n",
    "    #print(folder)\n",
    "    i += 1\n",
    "    if i == 11:\n",
    "        break\n",
    "    for file in os.listdir(directory+\"/\"+folder):\n",
    "        #print(file)\n",
    "        try:\n",
    "            (rate, sig) = wav.read(directory+\"/\"+folder+\"/\"+file)\n",
    "            mfcc_feat = mfcc(sig, rate, winlen = 0.020, appendEnergy=False)\n",
    "            covariance = np.cov(np.matrix.transpose(mfcc_feat))\n",
    "            mean_matrix = mfcc_feat.mean(0)\n",
    "            feature = (mean_matrix, covariance, i)\n",
    "            pickle.dump(feature, f)\n",
    "        # control if any exception occurs    \n",
    "        except Exception as e:\n",
    "            print(\"Got an exception: \", e, 'in folder: ', folder, ' filename: ', file)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4ab0b",
   "metadata": {},
   "source": [
    "### Test One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f7925",
   "metadata": {},
   "source": [
    "Implement a function that accepts a filename and copies all the data in form of a dataframe then randomly split the data into train and test sets so that we have a mix of different genres in both sets. I am using a random module and running a loop till the length of a dataset and generate a random fractional number between 0-1 and if it is less than 66 then a particular row is appended in the train test else in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "def loadDataset(filename, split, trset, teset):\n",
    "    with open('mydataset.dat','rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                dataset.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                f.close()\n",
    "                break\n",
    "    for x in range(len(dataset)):\n",
    "        if random.random() < split:\n",
    "            trset.append(dataset[x])\n",
    "        else:\n",
    "            teset.append(dataset[x])\n",
    "\n",
    "trainingSet = []\n",
    "testSet = []\n",
    "loadDataset('my.dat', 0.68, trainingSet, testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb51f9b",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc763889",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, s = librosa.load(\"./Data/genres_original\")\n",
    "\n",
    "print('y:', y, '\\n')\n",
    "print('y shape:', np.shape(y), '\\n')\n",
    "print('Sample Rate (KHz):', s, '\\n')\n",
    "\n",
    "print('Check Len of Audio:', 661794/22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, _ = librosa.effects.trim(y)\n",
    "\n",
    "print('Audio File:', audio, '\\n')\n",
    "print('Audio File shape:', np.shape(audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "librosa.display.waveshow(y = audio, sr = s, color = \"#00008B\");\n",
    "plt.title(\"Example Sound Waves on Blues\", fontsize = 23);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1239c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fourier Transform\n",
    "\n",
    "fft = 2048\n",
    "hl = 512\n",
    "\n",
    "stft = np.abs(librosa.stft(audio, n_fft = fft, hop_length = hl))\n",
    "\n",
    "print(np.shape(stft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 8))\n",
    "plt.plot(stft);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "decibel = librosa.amplitude_to_db(stft, ref = np.max)\n",
    "\n",
    "plt.figure(figsize = (16, 6))\n",
    "librosa.display.specshow(decibel, sr = s, hop_length = hl, x_axis = 'time', y_axis = 'log',\n",
    "                        cmap = 'cool')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad80fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, s = librosa.load(f'{dir_}/genres_original/disco/disco.00007.wav')\n",
    "y, z = librosa.effects.trim(y)\n",
    "\n",
    "\n",
    "mel = librosa.feature.melspectrogram(y, sr=s)\n",
    "mel_db = librosa.amplitude_to_db(mel, ref=np.max)\n",
    "plt.figure(figsize = (16, 6))\n",
    "librosa.display.specshow(mel_db, sr=s, hop_length=hl, x_axis = 'time', y_axis = 'log',\n",
    "                        cmap = 'bwr');\n",
    "plt.colorbar();\n",
    "plt.title(\"Disco Mel Spectrogram\", fontsize = 23);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e08f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, s = librosa.load(f'{dir_}/genres_original/jazz/jazz.00015.wav')\n",
    "y, z = librosa.effects.trim(y)\n",
    "\n",
    "\n",
    "mel = librosa.feature.melspectrogram(y, sr=s)\n",
    "mel_db = librosa.amplitude_to_db(mel, ref=np.max)\n",
    "plt.figure(figsize = (16, 6))\n",
    "librosa.display.specshow(mel_db, sr=s, hop_length=hl, x_axis = 'time', y_axis = 'log',\n",
    "                        cmap = 'bwr');\n",
    "plt.colorbar();\n",
    "plt.title(\"Jazz Mel Spectrogram\", fontsize = 23);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7035e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_cross = librosa.zero_crossings(audio, pad=False)\n",
    "print(sum(zero_cross))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a96e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_harm, y_perc = librosa.effects.hpss(audio)\n",
    "\n",
    "plt.figure(figsize = (16, 6))\n",
    "plt.plot(y_harm, color = '#028A0F');\n",
    "plt.plot(y_perc, color = '#FFB100');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo, _ = librosa.beat.beat_track(y, sr = s)\n",
    "tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = librosa.feature.spectral_centroid(audio, sr=s)[0]\n",
    "\n",
    "print('Centroids:', sc, '\\n')\n",
    "print('Shape of Spectral Centroids:', sc.shape, '\\n')\n",
    "\n",
    "frames = range(len(sc))\n",
    "\n",
    "t = librosa.frames_to_time(frames)\n",
    "\n",
    "print('frames:', frames, '\\n')\n",
    "print('t:', t)\n",
    "\n",
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "librosa.display.waveshow(audio, sr=s, alpha=0.4, color = '#028A0F');\n",
    "plt.plot(t, normalize(sc), color='#FFB100');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = librosa.feature.spectral_rolloff(audio, sr=s)[0]\n",
    "\n",
    "# The plot\n",
    "plt.figure(figsize = (16, 6))\n",
    "librosa.display.waveshow(audio, sr=s, alpha=0.4, color = '#028A0F');\n",
    "plt.plot(t, normalize(sr), color='#FFB100');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd366de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(audio, sr=s)\n",
    "print(mfcc.shape)\n",
    "\n",
    "plt.figure(figsize = (16, 6))\n",
    "librosa.display.specshow(mfcc, sr=s, x_axis='time', cmap = 'cool');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = sklearn.preprocessing.scale(mfcc, axis=1)\n",
    "print('Mean:', mfcc.mean(), '\\n')\n",
    "print('Var:', mfcc.var())\n",
    "\n",
    "plt.figure(figsize = (16, 6))\n",
    "librosa.display.specshow(mfcc, sr=s, x_axis='time', cmap = 'bwr');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hl = 5000\n",
    "\n",
    "#\n",
    "chromagram = librosa.feature.chroma_stft(audio, sr=s, hop_length=hl)\n",
    "print('Chromogram shape:', chromagram.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hl, cmap='bwr');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{dir_}/features_30_sec.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b19608",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike = [col for col in df.columns if 'mean' in col]\n",
    "corr = df[spike].corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(16, 11));\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=\"bwr\", vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.xticks(fontsize = 10)\n",
    "plt.yticks(fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e62c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[[\"label\", \"tempo\"]]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(16, 9));\n",
    "sns.boxplot(x = \"label\", y = \"tempo\", data = x, palette = 'summer');\n",
    "\n",
    "plt.title('BPM Boxplot for Genres', fontsize = 25)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 10);\n",
    "plt.xlabel(\"Genre\", fontsize = 15)\n",
    "plt.ylabel(\"BPM\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5839b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[0:, 1:]\n",
    "y = df['label']\n",
    "X = df.loc[:, df.columns != 'label']\n",
    "\n",
    "cols = X.columns\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "np_scaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(np_scaled, columns = cols)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "scaled_df = pca.fit_transform(X)\n",
    "df_p = pd.DataFrame(data = scaled_df, columns = ['pca1', 'pca2'])\n",
    "\n",
    "fdf = pd.concat([df_p, y], axis = 1)\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 9))\n",
    "sns.scatterplot(x = \"pca1\", y = \"pca2\", data = fdf, hue = \"label\", alpha = 0.7,\n",
    "               s = 100);\n",
    "\n",
    "plt.title('Genres with PCa', fontsize = 25)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 10);\n",
    "plt.xlabel(\"Principal Component 1\", fontsize = 15)\n",
    "plt.ylabel(\"Principal Component 2\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c0c8b",
   "metadata": {},
   "source": [
    "The distance function accepts two data points(X, and y coordinates) to calculate the actual distance between them. I used the numpy linear algebra package which provides a low-level implementation of standard linear algebra to find the dot product between the X-X and Y-Y coordinate of both points. To calculate the actual distance, we extract the determinant of the resultant array of both points and get the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(instance1, instance2, k):\n",
    "    distance = 0\n",
    "    mm1 = instance1[0]\n",
    "    cm1 = instance1[1]\n",
    "    mm2 = instance2[0]\n",
    "    cm2 = instance2[1]\n",
    "    distance = np.trace(np.dot(np.linalg.inv(cm2), cm1))\n",
    "    distance += (np.dot(np.dot((mm2-mm1).transpose(), np.linalg.inv(cm2)), mm2-mm1))\n",
    "    distance += np.log(np.linalg.det(cm2)) - np.log(np.linalg.det(cm1))\n",
    "    distance -= k\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b1f98c",
   "metadata": {},
   "source": [
    "We can use our KNN algorithm to get neighbors, extract class and check the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c922af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction using KNN(K nearest Neighbors)\n",
    "length = len(testSet)\n",
    "predictions = []\n",
    "for x in range(length):\n",
    "    predictions.append(nearestclass(getNeighbors(trainingSet, testSet[x], 5)))\n",
    "\n",
    "accuracy1 = getAccuracy(testSet, predictions)\n",
    "print(accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(folder)\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2603df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect an audio file from the dataset.\n",
    "\n",
    "sample_path = 'Data/genres_original/jazz/jazz.00000.wav'\n",
    "\n",
    "# if you want to listen to the audio, uncomment below.\n",
    "#display.Audio(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520133e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sample_rate = librosa.load(sample_path)\n",
    "\n",
    "print('y:', y, '\\n')\n",
    "print('y shape:', np.shape(y), '\\n')\n",
    "print('Sample rate (KHz):', sample_rate, '\\n')\n",
    "print(f'Length of audio: {np.shape(y)[0]/sample_rate}')\n",
    "\n",
    "# Plot th sound wave.\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(y=y, sr=sample_rate);\n",
    "plt.title(\"Sound wave of jazz.00000.wav\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sound wave to spectrogram.\n",
    "\n",
    "# Short-time Fourier transform (STFT).\n",
    "\n",
    "D = np.abs(librosa.stft(y, n_fft=2048, hop_length=512))\n",
    "print('Shape of D object:', np.shape(D))\n",
    "\n",
    "# Convert amplitude spectrogram to Decibels-scaled spectrogram.\n",
    "\n",
    "DB = librosa.amplitude_to_db(D, ref = np.max)\n",
    "\n",
    "# Creating the spectogram.\n",
    "\n",
    "plt.figure(figsize = (16, 6))\n",
    "librosa.display.specshow(DB, sr=sample_rate, hop_length=512,\n",
    "                         x_axis='time', y_axis='log')\n",
    "plt.colorbar()\n",
    "plt.title('Decibels-scaled spectrogram', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8cdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper functions (run me)\n",
    "\n",
    "def set_device():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "      print(\"WARNING: For this notebook to perform best, \"\n",
    "          \"if possible, in the menu under `Runtime` -> \"\n",
    "          \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "      print(\"GPU is enabled in this notebook.\")\n",
    "  return device\n",
    "\n",
    "\n",
    "def plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc):\n",
    "  epochs = len(train_loss)\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "  ax1.plot(list(range(epochs)), train_loss, label='Training Loss')\n",
    "  ax1.plot(list(range(epochs)), validation_loss, label='Validation Loss')\n",
    "  ax1.set_xlabel('Epochs')\n",
    "  ax1.set_ylabel('Loss')\n",
    "  ax1.set_title('Epoch vs Loss')\n",
    "  ax1.legend()\n",
    "\n",
    "  ax2.plot(list(range(epochs)), train_acc, label='Training Accuracy')\n",
    "  ax2.plot(list(range(epochs)), validation_acc, label='Validation Accuracy')\n",
    "  ax2.set_xlabel('Epochs')\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_title('Epoch vs Accuracy')\n",
    "  ax2.legend()\n",
    "  fig.set_size_inches(15.5, 5.5)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fdc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b943ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder with training, testing and validation data.\n",
    "\n",
    "spectrograms_dir = \"./Data/genres_original\"\"\n",
    "folder_names = ['Data/train/', 'Data/test/', 'Data/val/']\n",
    "train_dir = folder_names[0]\n",
    "test_dir = folder_names[1]\n",
    "val_dir = folder_names[2]\n",
    "\n",
    "for f in folder_names:\n",
    "  if os.path.exists(f):\n",
    "    shutil.rmtree(f)\n",
    "    os.mkdir(f)\n",
    "  else:\n",
    "    os.mkdir(f)\n",
    "\n",
    "# Loop over all genres.\n",
    "\n",
    "genres = list(os.listdir(spectrograms_dir))\n",
    "for g in genres:\n",
    "  # find all images & split in train, test, and validation\n",
    "  src_file_paths= []\n",
    "  for im in glob.glob(os.path.join(spectrograms_dir, f'{g}',\"*.png\"), recursive=True):\n",
    "    src_file_paths.append(im)\n",
    "  random.shuffle(src_file_paths)\n",
    "  test_files = src_file_paths[0:10]\n",
    "  val_files = src_file_paths[10:20]\n",
    "  train_files = src_file_paths[20:]\n",
    "\n",
    "  #  make destination folders for train and test images\n",
    "  for f in folder_names:\n",
    "    if not os.path.exists(os.path.join(f + f\"{g}\")):\n",
    "      os.mkdir(os.path.join(f + f\"{g}\"))\n",
    "\n",
    "  # copy training and testing images over\n",
    "  for f in train_files:\n",
    "    shutil.copy(f, os.path.join(os.path.join(train_dir + f\"{g}\") + '/',os.path.split(f)[1]))\n",
    "  for f in test_files:\n",
    "    shutil.copy(f, os.path.join(os.path.join(test_dir + f\"{g}\") + '/',os.path.split(f)[1]))\n",
    "  for f in val_files:\n",
    "    shutil.copy(f, os.path.join(os.path.join(val_dir + f\"{g}\") + '/',os.path.split(f)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13dcda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    train_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=25, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    val_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=25, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a CNN & train it to predict genres.\n",
    "\n",
    "class music_net(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"Intitalize neural net layers\"\"\"\n",
    "    super(music_net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.fc1 = nn.Linear(in_features=9856, out_features=10)\n",
    "\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=8)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=16)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(num_features=32)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "    self.dropout = nn.Dropout(p=0.3, inplace=False)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Conv layer 1.\n",
    "    x = self.conv1(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 2.\n",
    "    x = self.conv2(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 3.\n",
    "    x = self.conv3(x)\n",
    "    x = self.batchnorm3(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 4.\n",
    "    x = self.conv4(x)\n",
    "    x = self.batchnorm4(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 5.\n",
    "    x = self.conv5(x)\n",
    "    x = self.batchnorm5(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Fully connected layer 1.\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc1(x)\n",
    "    x = F.softmax(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, validation_loader, epochs):\n",
    "  criterion =  nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "  train_loss, validation_loss = [], []\n",
    "  train_acc, validation_acc = [], []\n",
    "  with tqdm(range(epochs), unit='epoch') as tepochs:\n",
    "    tepochs.set_description('Training')\n",
    "    for epoch in tepochs:\n",
    "      model.train()\n",
    "      # keep track of the running loss\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in train_loader:\n",
    "        # getting the training set\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Get the model output (call the model with the data from this batch)\n",
    "        output = model(data)\n",
    "        # Zero the gradients out)\n",
    "        optimizer.zero_grad()\n",
    "        # Get the Loss\n",
    "        loss  = criterion(output, target)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights (using the training step of the optimizer)\n",
    "        optimizer.step()\n",
    "\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss  # add the loss for this batch\n",
    "\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      # append the loss for this epoch\n",
    "      train_loss.append(running_loss.detach().cpu().item()/len(train_loader))\n",
    "      train_acc.append(correct/total)\n",
    "\n",
    "      # evaluate on validation data\n",
    "      model.eval()\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in validation_loader:\n",
    "        # getting the validation set\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss.item()\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      validation_loss.append(running_loss/len(validation_loader))\n",
    "      validation_acc.append(correct/total)\n",
    "\n",
    "  return train_loss, train_acc, validation_loss, validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c73d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training.\n",
    "\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
    "\n",
    "# Detach tensors from GPU\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94784b68",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librosa is a Python module that analyzes audio signals like music\n",
    "import librosa\n",
    "\n",
    "# load audio file\n",
    "audio_path = './Data/genres_original/rock/rock.00099.wav'\n",
    "x , sr = librosa.load(audio_path)\n",
    "\n",
    "# Returns an audio time series as a numpy array with a default sampling rate(sr) of 22KHZ mono\n",
    "print(type(x), type(sr))\n",
    "\n",
    "print(x.shape, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45522b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython lets plays audio directly in a jupyter notebook \n",
    "import IPython.display as ipd\n",
    "\n",
    "# play audio clip\n",
    "ipd.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the amplitude envelope of a waveform using the audio array\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(x, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfcc9b",
   "metadata": {},
   "source": [
    "A spectrogram is a visual representation of the spectrum of frequencies of sound over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e194215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "%matplotlib inline\n",
    "\n",
    "X = librosa.stft(x)\n",
    "Xdb = librosa.amplitude_to_db(abs(X))\n",
    "# Vertical axis shows frequencies (from 0 to 10kHz), and the horizontal axis shows the time of the clip\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87724a3",
   "metadata": {},
   "source": [
    "A spectral centriod indicates where the \"center of mass\" for a sound is located and is calculated as the weighted mean of the frequencies present in the sound. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "# Load the signal\n",
    "x, sr = librosa.load('./Data/genres_original/rock/rock.00099.wav')\n",
    "\n",
    "\n",
    "spectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]\n",
    "spectral_centroids.shape\n",
    "# Computing the time variable for visualization\n",
    "frames = range(len(spectral_centroids))\n",
    "t = librosa.frames_to_time(frames)\n",
    "# Normalising the spectral centroid for visualisation\n",
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
    "#Plotting the Spectral Centroid along the waveform\n",
    "librosa.display.waveshow(x, sr=sr, alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_centroids), color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b14eb",
   "metadata": {},
   "source": [
    "Spectral Rolloff is a measure of the shape of the audio signal. It represents the frequency below which a specified percentage of the total spectral energy, e.g., 85%, lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a930c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_rolloff = librosa.feature.spectral_rolloff(x+0.01, sr=sr)[0]\n",
    "librosa.display.waveshow(x, sr=sr, alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_rolloff), color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5a31b",
   "metadata": {},
   "source": [
    "The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10â€“20) that concisely describe the overall shape of a spectral envelope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes MFCCs across an audio signal\n",
    "x, fs = librosa.load('./Data/genres_original/rock/rock.00099.wav')\n",
    "mfccs = librosa.feature.mfcc(x, sr=fs)\n",
    "print (mfccs.shape)\n",
    "\n",
    "#Displaying  the MFCCs:\n",
    "librosa.display.specshow(mfccs, sr=sr, x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature scaling such that each coefficient dimension has zero mean and unit variance\n",
    "mfccs = sklearn.preprocessing.scale(mfccs, axis=1)\n",
    "print(mfccs.mean(axis=1))\n",
    "print(mfccs.var(axis=1))\n",
    "librosa.display.specshow(mfccs, sr=sr, x_axis='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058c5d9",
   "metadata": {},
   "source": [
    "Chroma frequencies project the entire spectrum onto 12 bins representing the 12 distinct semitones (or chroma) of the musical octave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8713dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length = 512\n",
    "chromagram = librosa.feature.chroma_stft(x, sr=sr, hop_length=hop_length)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fa900",
   "metadata": {},
   "source": [
    "After implementing and training our KNN model and visualizing the resulting audio file in multiple ways we can check for the new data to check how much our model is compliant in predicting the new audio file. All the genres (or classes) are in numeric form, and we need to check the class name so first, we will implement a dictionary where the key is numeric label and value is the category name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "results = defaultdict(int)\n",
    "\n",
    "directory = \"./Data/genres_original\"\n",
    "\n",
    "i = 1\n",
    "for folder in os.listdir(directory):\n",
    "    results[i] = folder\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76908964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict a new audio file and receive a label for it and print the name of the category using the result dictionary.\n",
    "pred = nearestclass(getNeighbors(dataset, feature, 5))\n",
    "print(results[pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fccb25a",
   "metadata": {},
   "source": [
    "### Test Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed70afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "def loadDataset(filename, split, trset, teset):\n",
    "    with open('mydataset.dat','rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                dataset.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                f.close()\n",
    "                break\n",
    "    for x in range(len(dataset)):\n",
    "        if random.random() < split:\n",
    "            trset.append(dataset[x])\n",
    "        else:\n",
    "            teset.append(dataset[x])\n",
    "\n",
    "trainingSet = []\n",
    "testSet = []\n",
    "loadDataset('my.dat', 0.68, trainingSet, testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(instance1, instance2, k):\n",
    "    distance = 0\n",
    "    mm1 = instance1[0]\n",
    "    cm1 = instance1[1]\n",
    "    mm2 = instance2[0]\n",
    "    cm2 = instance2[1]\n",
    "    distance = np.trace(np.dot(np.linalg.inv(cm2), cm1))\n",
    "    distance += (np.dot(np.dot((mm2-mm1).transpose(), np.linalg.inv(cm2)), mm2-mm1))\n",
    "    distance += np.log(np.linalg.det(cm2)) - np.log(np.linalg.det(cm1))\n",
    "    distance -= k\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction using KNN(K nearest Neighbors)\n",
    "length = len(testSet)\n",
    "predictions = []\n",
    "for x in range(length):\n",
    "    predictions.append(nearestclass(getNeighbors(trainingSet, testSet[x], 5)))\n",
    "\n",
    "accuracy2 = getAccuracy(testSet, predictions)\n",
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a09157",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(folder)\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "audio_path2 = './Data/genres_original/country/country.00013.wav'\n",
    "x , sr = librosa.load(audio_path2)\n",
    "\n",
    "print(type(x), type(sr))\n",
    "\n",
    "print(x.shape, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286eb374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "# play audio clip\n",
    "ipd.Audio(audio_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958bfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(x, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "%matplotlib inline\n",
    "\n",
    "X = librosa.stft(x)\n",
    "Xdb = librosa.amplitude_to_db(abs(X))\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da04b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "# Load the signal\n",
    "x, sr = librosa.load('./Data/genres_original/country/country.00013.wav')\n",
    "\n",
    "\n",
    "spectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]\n",
    "spectral_centroids.shape\n",
    "(775,)\n",
    "# Computing the time variable for visualization\n",
    "frames = range(len(spectral_centroids))\n",
    "t = librosa.frames_to_time(frames)\n",
    "# Normalising the spectral centroid for visualisation\n",
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
    "#Plotting the Spectral Centroid along the waveform\n",
    "librosa.display.waveshow(x, sr=sr, alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_centroids), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c635351",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_rolloff = librosa.feature.spectral_rolloff(x+0.01, sr=sr)[0]\n",
    "librosa.display.waveshow(x, sr=sr, alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_rolloff), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa11f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, fs = librosa.load('./Data/genres_original/country/country.00013.wav')\n",
    "mfccs = librosa.feature.mfcc(x, sr=fs)\n",
    "print (mfccs.shape)\n",
    "\n",
    "#Displaying  the MFCCs:\n",
    "librosa.display.specshow(mfccs, sr=sr, x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = sklearn.preprocessing.scale(mfccs, axis=1)\n",
    "print(mfccs.mean(axis=1))\n",
    "print(mfccs.var(axis=1))\n",
    "librosa.display.specshow(mfccs, sr=sr, x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94308d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length = 512\n",
    "chromagram = librosa.feature.chroma_stft(x, sr=sr, hop_length=hop_length)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52adfb1c",
   "metadata": {},
   "source": [
    "### CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de648c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper functions (run me)\n",
    "\n",
    "def set_device():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "      print(\"WARNING: For this notebook to perform best, \"\n",
    "          \"if possible, in the menu under `Runtime` -> \"\n",
    "          \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "      print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "def plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc):\n",
    "  epochs = len(train_loss)\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "  ax1.plot(list(range(epochs)), train_loss, label='Training Loss')\n",
    "  ax1.plot(list(range(epochs)), validation_loss, label='Validation Loss')\n",
    "  ax1.set_xlabel('Epochs')\n",
    "  ax1.set_ylabel('Loss')\n",
    "  ax1.set_title('Epoch vs Loss')\n",
    "  ax1.legend()\n",
    "\n",
    "  ax2.plot(list(range(epochs)), train_acc, label='Training Accuracy')\n",
    "  ax2.plot(list(range(epochs)), validation_acc, label='Validation Accuracy')\n",
    "  ax2.set_xlabel('Epochs')\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_title('Epoch vs Accuracy')\n",
    "  ax2.legend()\n",
    "  fig.set_size_inches(15.5, 5.5)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42117f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642fad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder with training, testing and validation data.\n",
    "\n",
    "spectrograms_dir = \"Data/images_original/\"\n",
    "folder_names = ['Data/train/', 'Data/test/', 'Data/val/']\n",
    "train_dir = folder_names[0]\n",
    "test_dir = folder_names[1]\n",
    "val_dir = folder_names[2]\n",
    "\n",
    "for f in folder_names:\n",
    "  if os.path.exists(f):\n",
    "    shutil.rmtree(f)\n",
    "    os.mkdir(f)\n",
    "  else:\n",
    "    os.mkdir(f)\n",
    "    \n",
    "# Loop over all genres.\n",
    "\n",
    "genres = list(os.listdir(spectrograms_dir))\n",
    "for g in genres:\n",
    "  # find all images & split in train, test, and validation\n",
    "  src_file_paths= []\n",
    "  for im in glob.glob(os.path.join(spectrograms_dir, f'{g}',\"*.png\"), recursive=True):\n",
    "    src_file_paths.append(im)\n",
    "  random.shuffle(src_file_paths)\n",
    "  test_files = src_file_paths[0:10]\n",
    "  val_files = src_file_paths[10:20]\n",
    "  train_files = src_file_paths[20:]\n",
    "\n",
    "  #  make destination folders for train and test images\n",
    "  for f in folder_names:\n",
    "    if not os.path.exists(os.path.join(f + f\"{g}\")):\n",
    "      os.mkdir(os.path.join(f + f\"{g}\"))\n",
    "\n",
    "  # copy training and testing images over\n",
    "  for f in train_files:\n",
    "    shutil.copy(f, os.path.join(os.path.join(train_dir + f\"{g}\") + '/',os.path.split(f)[1]))\n",
    "  for f in test_files:\n",
    "    shutil.copy(f, os.path.join(os.path.join(test_dir + f\"{g}\") + '/',os.path.split(f)[1]))\n",
    "  for f in val_files:\n",
    "    shutil.copy(f, os.path.join(os.path.join(val_dir + f\"{g}\") + '/',os.path.split(f)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading.\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    train_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=25, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    val_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=25, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26488a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a CNN & train it to predict genres.\n",
    "\n",
    "class music_net(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"Intitalize neural net layers\"\"\"\n",
    "    super(music_net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.fc1 = nn.Linear(in_features=9856, out_features=10)\n",
    "\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=8)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=16)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(num_features=32)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "    self.dropout = nn.Dropout(p=0.3, inplace=False)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Conv layer 1.\n",
    "    x = self.conv1(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 2.\n",
    "    x = self.conv2(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 3.\n",
    "    x = self.conv3(x)\n",
    "    x = self.batchnorm3(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 4.\n",
    "    x = self.conv4(x)\n",
    "    x = self.batchnorm4(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 5.\n",
    "    x = self.conv5(x)\n",
    "    x = self.batchnorm5(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Fully connected layer 1.\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc1(x)\n",
    "    x = F.softmax(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, validation_loader, epochs):\n",
    "  criterion =  nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "  train_loss, validation_loss = [], []\n",
    "  train_acc, validation_acc = [], []\n",
    "  with tqdm(range(epochs), unit='epoch') as tepochs:\n",
    "    tepochs.set_description('Training')\n",
    "    for epoch in tepochs:\n",
    "      model.train()\n",
    "      # keep track of the running loss\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in train_loader:\n",
    "        # getting the training set\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Get the model output (call the model with the data from this batch)\n",
    "        output = model(data)\n",
    "        # Zero the gradients out)\n",
    "        optimizer.zero_grad()\n",
    "        # Get the Loss\n",
    "        loss  = criterion(output, target)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights (using the training step of the optimizer)\n",
    "        optimizer.step()\n",
    "\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss  # add the loss for this batch\n",
    "              # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      # append the loss for this epoch\n",
    "      train_loss.append(running_loss.detach().cpu().item()/len(train_loader))\n",
    "      train_acc.append(correct/total)\n",
    "\n",
    "      # evaluate on validation data\n",
    "      model.eval()\n",
    "      running_loss = 0.\n",
    "      correct, total = 0, 0\n",
    "\n",
    "      for data, target in validation_loader:\n",
    "        # getting the validation set\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        tepochs.set_postfix(loss=loss.item())\n",
    "        running_loss += loss.item()\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "      validation_loss.append(running_loss/len(validation_loader))\n",
    "      validation_acc.append(correct/total)\n",
    "\n",
    "  return train_loss, train_acc, validation_loss, validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training.\n",
    "\n",
    "net = music_net().to(device)\n",
    "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
    "\n",
    "# Detach tensors from GPU\n",
    "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a64a87",
   "metadata": {},
   "source": [
    "### CNN-RNN Tandem Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eee04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 8\n",
    "n_features = X_train.shape[2]\n",
    "n_time = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LAYERS = 3\n",
    "FILTER_LENGTH = 5\n",
    "CONV_FILTER_COUNT = 56\n",
    "BATCH_SIZE = 32\n",
    "LSTM_COUNT = 96\n",
    "EPOCH_COUNT = 70\n",
    "NUM_HIDDEN = 64\n",
    "L2_regularization = 0.001\n",
    "\n",
    "def conv_recurrent_model_build(model_input):\n",
    "    print('Building model...')\n",
    "    layer = model_input\n",
    "    \n",
    "    ### 3 1D Convolution Layers\n",
    "    for i in range(N_LAYERS):\n",
    "        # give name to the layers\n",
    "        layer = Conv1D(\n",
    "                filters=CONV_FILTER_COUNT,\n",
    "                kernel_size=FILTER_LENGTH,\n",
    "                kernel_regularizer=regularizers.l2(L2_regularization),  # Tried 0.001\n",
    "                name='convolution_' + str(i + 1)\n",
    "            )(layer)\n",
    "        layer = BatchNormalization(momentum=0.9)(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "        layer = MaxPooling1D(2)(layer)\n",
    "        layer = Dropout(0.4)(layer)\n",
    "    \n",
    "    ## LSTM Layer\n",
    "    layer = LSTM(LSTM_COUNT, return_sequences=False)(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    \n",
    "    ## Dense Layer\n",
    "    layer = Dense(NUM_HIDDEN, kernel_regularizer=regularizers.l2(L2_regularization), name='dense1')(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    \n",
    "    ## Softmax Output\n",
    "    layer = Dense(num_classes)(layer)\n",
    "    layer = Activation('softmax', name='output_realtime')(layer)\n",
    "    model_output = layer\n",
    "    model = Model(model_input, model_output)\n",
    "    \n",
    "    \n",
    "    opt = Adam(lr=0.001)\n",
    "    model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cd1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, x_val, y_val):\n",
    "    \n",
    "    n_features = x_train.shape[2]\n",
    "    input_shape = (None, n_features)\n",
    "    model_input = Input(input_shape, name='input')\n",
    "    \n",
    "    model = conv_recurrent_model_build(model_input)\n",
    "    \n",
    "#     tb_callback = TensorBoard(log_dir='./logs/4', histogram_freq=1, batch_size=32, write_graph=True, write_grads=False,\n",
    "#                               write_images=False, embeddings_freq=0, embeddings_layer_names=None,\n",
    "#                               embeddings_metadata=None)\n",
    "    checkpoint_callback = ModelCheckpoint('./models/crnn/weights.best.h5', monitor='val_acc', verbose=1,\n",
    "                                          save_best_only=True, mode='max')\n",
    "    \n",
    "    reducelr_callback = ReduceLROnPlateau(\n",
    "                monitor='val_acc', factor=0.5, patience=10, min_delta=0.01,\n",
    "                verbose=1\n",
    "            )\n",
    "    callbacks_list = [checkpoint_callback, reducelr_callback]\n",
    "\n",
    "    # Fit the model and get training history.\n",
    "    print('Training...')\n",
    "    history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCH_COUNT,\n",
    "                        validation_data=(x_val, y_val), verbose=1, callbacks=callbacks_list)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary_stats(history):\n",
    "    # List all data in history\n",
    "    print(history.history.keys())\n",
    "\n",
    "    # Summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history  = train_model(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6531c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_summary_stats(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = np.argmax(y_valid, axis = 1)\n",
    "y_pred = model.predict(X_valid)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "labels = [0,1,2,3,4,5,6,7]\n",
    "target_names = dict_genres.keys()\n",
    "\n",
    "print(y_true.shape, y_pred.shape)\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de078e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "weights_path = 'models/crnn/weights.best.h5'\n",
    "model = load_model(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Afile = np.load('test_arr.npz')\n",
    "print(Afile.files)\n",
    "X_test = Afile['arr_0']\n",
    "y_test = Afile['arr_1']\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f60c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test -= 1\n",
    "print(np.amin(y_test), np.amax(y_test), np.mean(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bdb241",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_raw = librosa.core.db_to_power(X_test, ref=1.0)\n",
    "print(np.amin(X_test_raw), np.amax(X_test_raw), np.mean(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63022fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.log(X_test_raw)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "labels = [0,1,2,3,4,5,6,7]\n",
    "target_names = dict_genres.keys()\n",
    "\n",
    "print(y_true.shape, y_pred.shape)\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = keras.utils.to_categorical(y_test, num_classes=8)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefa817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1bcaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10 clusters\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "mat = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=dict_genres.keys(),\n",
    "            yticklabels=dict_genres.keys())\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93d21b",
   "metadata": {},
   "source": [
    "### CNN-RNN Paralell Implemenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c17708",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Afile['arr_0']\n",
    "y_train = Afile['arr_1']\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f80f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 8\n",
    "n_features = X_train.shape[2]\n",
    "n_time = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c964685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filters1=16 \n",
    "nb_filters2=32 \n",
    "nb_filters3=64\n",
    "nb_filters4=64\n",
    "nb_filters5=64\n",
    "ksize = (3,1)\n",
    "pool_size_1= (2,2) \n",
    "pool_size_2= (4,4)\n",
    "pool_size_3 = (4,2)\n",
    "\n",
    "dropout_prob = 0.20\n",
    "dense_size1 = 128\n",
    "lstm_count = 64\n",
    "num_units = 120\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCH_COUNT = 50\n",
    "L2_regularization = 0.001\n",
    "\n",
    "def conv_recurrent_model_build(model_input):\n",
    "    print('Building model...')\n",
    "    layer = model_input\n",
    "    \n",
    "    ### Convolutional blocks\n",
    "    conv_1 = Conv2D(filters = nb_filters1, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_1')(layer)\n",
    "    pool_1 = MaxPooling2D(pool_size_1)(conv_1)\n",
    "\n",
    "    conv_2 = Conv2D(filters = nb_filters2, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_2')(pool_1)\n",
    "    pool_2 = MaxPooling2D(pool_size_1)(conv_2)\n",
    "\n",
    "    conv_3 = Conv2D(filters = nb_filters3, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_3')(pool_2)\n",
    "    pool_3 = MaxPooling2D(pool_size_1)(conv_3)\n",
    "    \n",
    "    \n",
    "    conv_4 = Conv2D(filters = nb_filters4, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_4')(pool_3)\n",
    "    pool_4 = MaxPooling2D(pool_size_2)(conv_4)\n",
    "    \n",
    "    \n",
    "    conv_5 = Conv2D(filters = nb_filters5, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_5')(pool_4)\n",
    "    pool_5 = MaxPooling2D(pool_size_2)(conv_5)\n",
    "\n",
    "    flatten1 = Flatten()(pool_5)\n",
    "    ### Recurrent Block\n",
    "    \n",
    "    # Pooling layer\n",
    "    pool_lstm1 = MaxPooling2D(pool_size_3, name = 'pool_lstm')(layer)\n",
    "    \n",
    "    # Embedding layer\n",
    "\n",
    "    squeezed = Lambda(lambda x: K.squeeze(x, axis= -1))(pool_lstm1)\n",
    "#     flatten2 = K.squeeze(pool_lstm1, axis = -1)\n",
    "#     dense1 = Dense(dense_size1)(flatten)\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    lstm = Bidirectional(GRU(lstm_count))(squeezed)  #default merge mode is concat\n",
    "    \n",
    "    # Concat Output\n",
    "    concat = concatenate([flatten1, lstm], axis=-1, name ='concat')\n",
    "    \n",
    "    ## Softmax Output\n",
    "    output = Dense(num_classes, activation = 'softmax', name='preds')(concat)\n",
    "    \n",
    "    model_output = output\n",
    "    model = Model(model_input, model_output)\n",
    "    \n",
    "#     opt = Adam(lr=0.001)\n",
    "    opt = RMSprop(lr=0.0005)  # Optimizer\n",
    "    model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca36461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, x_val, y_val):\n",
    "    \n",
    "    n_frequency = 128\n",
    "    n_frames = 640\n",
    "    #reshape and expand dims for conv2d\n",
    "#     x_train = x_train.reshape(-1, n_frequency, n_frames)\n",
    "    x_train = np.expand_dims(x_train, axis = -1)\n",
    "    \n",
    "#     x_val = x_val.reshape(-1, n_frequency, n_frames)\n",
    "    x_val = np.expand_dims(x_val, axis = -1)\n",
    "    \n",
    "    \n",
    "    input_shape = (n_frames, n_frequency, 1)\n",
    "    model_input = Input(input_shape, name='input')\n",
    "    \n",
    "    model = conv_recurrent_model_build(model_input)\n",
    "    \n",
    "#     tb_callback = TensorBoard(log_dir='./logs/4', histogram_freq=1, batch_size=32, write_graph=True, write_grads=False,\n",
    "#                               write_images=False, embeddings_freq=0, embeddings_layer_names=None,\n",
    "#                               embeddings_metadata=None)\n",
    "    checkpoint_callback = ModelCheckpoint('./models/parallel/weights.best.h5', monitor='val_acc', verbose=1,\n",
    "                                          save_best_only=True, mode='max')\n",
    "    \n",
    "    reducelr_callback = ReduceLROnPlateau(\n",
    "                monitor='val_acc', factor=0.5, patience=10, min_delta=0.01,\n",
    "                verbose=1\n",
    "            )\n",
    "    callbacks_list = [checkpoint_callback, reducelr_callback]\n",
    "\n",
    "    # Fit the model and get training history.\n",
    "    print('Training...')\n",
    "    history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCH_COUNT,\n",
    "                        validation_data=(x_val, y_val), verbose=1, callbacks=callbacks_list)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary_stats(history):\n",
    "    # List all data in history\n",
    "    print(history.history.keys())\n",
    "\n",
    "    # Summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec338013",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history  = train_model(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_summary_stats(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70254be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = np.argmax(y_valid, axis = 1)\n",
    "X_valid = np.expand_dims(X_valid, axis = -1)\n",
    "y_pred = model.predict(X_valid)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "labels = [0,1,2,3,4,5,6,7]\n",
    "target_names = dict_genres.keys()\n",
    "\n",
    "print(y_true.shape, y_pred.shape)\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f823e45",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169150d9",
   "metadata": {},
   "source": [
    "The most effective method for sorting our music genres among KNN, CNN alone, and CNN run with RNN in both parallel and tandem is by far CNN. After training for 70 epochs, I got training accuracy of 99.57%, and on validation 89.03%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
